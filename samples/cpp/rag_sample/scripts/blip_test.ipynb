{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chuxd\\miniconda3\\envs\\ov_rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # for reproducibility\n",
    "\n",
    "# Set the seed to fix the projection layer weights\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-vqa-base and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipModel.from_pretrained(\"Salesforce/blip-vqa-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['pixel_values']: torch.Size([1, 3, 384, 384])\n",
      "torch.Size([1, 512])\n",
      "tensor([[ 5.4370e-01, -1.0282e-01, -4.2045e-01, -7.7390e-03, -7.3555e-02,\n",
      "         -7.7148e-02, -4.5607e-02,  3.9742e-01,  1.7652e-01, -2.0652e-01,\n",
      "          2.7597e-01, -4.1239e-02,  3.3221e-01,  1.5150e-01, -6.3844e-03,\n",
      "         -1.3276e-01, -1.0011e-01,  1.5942e-01, -6.2723e-02, -3.5384e-01,\n",
      "         -1.2362e-01,  6.3965e-01,  4.1510e-01, -9.9639e-01,  2.9846e-01,\n",
      "          2.3616e-01, -5.8012e-01,  8.0132e-01, -3.5859e-01, -1.0994e-01,\n",
      "         -3.1600e-01, -6.7240e-01, -6.9920e-01, -7.9811e-02,  1.4866e-02,\n",
      "         -1.4781e-02,  1.6229e-01,  3.8690e-01,  1.6235e-02, -2.8352e-01,\n",
      "         -3.0121e-01, -2.0102e-01,  1.5626e-01, -1.7362e-01, -2.3560e-01,\n",
      "         -1.4569e-01,  3.1506e-01,  6.9732e-01,  3.4207e-02,  9.4353e-02,\n",
      "          5.4746e-01,  1.3270e-01, -1.1987e-01, -2.6232e-01, -2.3362e-01,\n",
      "         -5.1732e-01,  3.0591e-01,  5.6168e-01, -5.5495e-01,  1.9668e-02,\n",
      "         -1.5082e-01,  4.3294e-01,  3.4502e-01, -1.8231e-01,  1.0135e-02,\n",
      "         -2.3864e-01,  5.4490e-01,  2.9734e-01,  9.6388e-02, -1.4328e-01,\n",
      "          3.2253e-01, -3.5973e-01, -2.0221e-01, -1.9357e-01,  4.3506e-02,\n",
      "          9.0290e-03, -4.2408e-02, -2.6150e-01, -3.1127e-01, -8.8364e-01,\n",
      "         -1.9187e-01, -1.0726e-01, -3.6147e-02,  2.3196e-02, -4.3753e-02,\n",
      "         -3.1864e-01, -3.3567e-01, -4.8086e-01,  7.8214e-02, -3.5744e-01,\n",
      "         -4.4473e-01, -2.6328e-01, -4.4784e-01, -2.5614e-01,  6.0475e-02,\n",
      "         -7.0908e-02,  2.7063e-01,  9.2749e-02,  1.4245e-01, -9.9973e-01,\n",
      "          3.9866e-01, -2.2048e-01,  2.6616e-01, -2.7032e-01,  7.6080e-02,\n",
      "         -2.4097e-01,  2.3215e-01, -2.2765e-02,  6.1126e-01,  5.4347e-03,\n",
      "          5.2923e-01, -1.5295e-01,  7.7672e-02,  3.2874e-01, -2.7363e-01,\n",
      "         -4.7545e-01, -3.3780e-01, -7.0484e-02, -2.3679e-01,  3.8032e-01,\n",
      "         -1.3434e-01,  2.3164e-01,  1.5858e-01,  8.2171e-02, -4.0384e-02,\n",
      "         -4.5346e-01, -2.3410e-01, -1.4820e-01,  1.7085e-01, -2.1513e-03,\n",
      "         -3.9179e-01, -3.4470e-01, -7.4864e-01,  1.3955e-01, -9.7516e-02,\n",
      "          6.7973e-01, -6.1376e-02,  9.9601e-02, -3.5855e-01,  9.2985e-02,\n",
      "          5.5162e-02, -3.8796e-01, -4.0954e-01,  2.3903e-01, -5.8879e-01,\n",
      "          4.9370e-01,  3.0742e-01, -7.3794e-02,  6.6626e-02,  3.0190e-01,\n",
      "         -4.9463e-01,  1.4314e-01,  4.4949e-01, -7.4217e-01, -7.2915e-01,\n",
      "          9.3777e-02,  5.8411e-02, -3.1938e-01, -4.3174e-01, -2.7459e-01,\n",
      "         -3.5500e-01,  1.6612e-01,  3.9076e-01,  5.6233e-01,  1.0057e-01,\n",
      "         -1.1183e-01, -4.7837e-02, -2.6376e-01,  1.1936e-01,  6.9802e-01,\n",
      "          3.5151e-01,  6.4126e-01,  1.4376e-01, -5.7223e-01,  1.8769e-01,\n",
      "         -6.5051e-01,  1.2139e-01,  1.9161e-01, -4.7534e-02,  4.5866e-04,\n",
      "          3.0891e-01, -2.6543e-01, -1.2993e-01,  1.9766e-01,  1.8491e-01,\n",
      "         -4.5500e-01,  3.3759e-01,  3.1690e-01,  1.0871e-01,  3.3625e-01,\n",
      "         -3.3473e-01,  3.1673e-01, -2.6564e-01, -4.6514e-02, -9.8157e-02,\n",
      "          2.1249e-01, -5.2224e-02,  2.2309e-01, -2.0412e-02,  4.3286e-01,\n",
      "         -9.2707e-02,  1.1708e+00,  5.1251e-01,  4.1079e-02,  1.3786e-01,\n",
      "         -4.8222e-01,  4.0155e-01,  1.6674e-01,  3.4366e-02,  2.9073e-01,\n",
      "          2.3014e-01, -5.2751e-02,  3.4966e-01,  1.0090e-01, -9.7606e-02,\n",
      "          2.8413e-01,  1.6938e-01,  3.0105e-01, -6.8609e-02,  2.6620e-01,\n",
      "          5.2929e-02, -1.3112e-01,  3.7653e-01, -9.1568e-02,  3.2316e-01,\n",
      "          1.5219e-01,  4.3095e-01, -8.2745e-01,  4.7855e-01, -3.6357e-01,\n",
      "          3.5269e-01, -3.0365e-01,  2.5468e-01,  6.7027e-01, -3.4112e-01,\n",
      "          4.5601e-02, -1.5073e-01, -7.2671e-01, -4.4130e-01,  1.7187e-02,\n",
      "          1.6578e-01, -1.7686e-01, -9.4867e-02, -4.4471e-01,  8.7288e-02,\n",
      "         -4.6895e-01,  4.7017e-01, -6.5509e-01, -2.7036e-01,  5.3257e-01,\n",
      "          1.9580e-01,  1.6335e-01, -6.9786e-01, -1.8738e-01, -3.0252e-01,\n",
      "          3.4624e-01,  2.2003e-01, -1.8199e-01,  1.0620e-01, -3.6548e-01,\n",
      "          3.2032e-01, -1.0287e-01, -4.9057e-01,  7.4077e-02, -4.7852e-01,\n",
      "          1.2622e-01,  1.5142e-01, -2.0690e-01, -1.4345e-02,  2.0661e-01,\n",
      "         -9.9898e-02,  7.3921e-01, -5.6192e-02,  2.4079e-01, -1.0815e-01,\n",
      "         -1.0476e-01,  8.5874e-01,  1.0014e-01,  1.3026e-01, -2.0582e-01,\n",
      "          1.8891e-01,  3.9730e-02, -7.9765e-01, -3.3502e-01, -1.9047e-01,\n",
      "         -1.7510e-02, -2.3701e-03, -1.2032e-01, -3.8277e-01,  1.8530e-01,\n",
      "         -4.4104e-01,  6.6947e-01, -3.5067e-02, -6.2751e-02,  4.2569e-01,\n",
      "          4.6692e-01, -3.9054e-02,  5.1116e-01,  6.3713e-01, -2.6041e-01,\n",
      "          6.5197e-02,  4.3360e-03, -1.1186e-01,  5.0277e-01, -2.3119e-01,\n",
      "          1.3751e-01, -3.5145e-01, -5.0685e-01, -3.5549e-01,  1.4979e-01,\n",
      "          2.6842e-01,  2.1720e-01, -4.5272e-02,  2.2385e-01,  4.2779e-01,\n",
      "         -2.4480e-01,  2.1757e-01,  1.6869e-02, -3.7008e-01,  6.5847e-02,\n",
      "         -6.9611e-01, -4.3476e-01, -1.1100e-01,  7.7715e-01,  2.2815e-01,\n",
      "         -3.4789e-01,  1.1658e-01, -2.1210e-01, -1.4893e-02,  1.1489e-01,\n",
      "          5.3004e-01,  1.5088e-01,  5.6640e-03,  6.0866e-03, -6.5775e-01,\n",
      "          1.2523e-01, -2.9789e-01,  5.7492e-02, -4.9785e-01, -3.6636e-01,\n",
      "         -2.9969e-01, -4.7038e-01, -1.3765e-01, -3.9175e-01, -4.2835e-01,\n",
      "          1.6289e-01, -5.2548e-01,  1.9187e-01,  2.5599e-01,  1.6434e-01,\n",
      "         -2.0063e-02, -3.0431e-01, -1.0954e-01, -3.4684e-01,  1.2494e-01,\n",
      "          2.3792e-01,  2.8831e-01, -5.9036e-01,  9.9303e-02, -3.4395e-01,\n",
      "         -4.9470e-02,  7.3448e-02, -1.9168e-01, -2.8135e-01,  5.0598e-02,\n",
      "         -1.0336e-02,  3.4850e-01, -1.1240e-01, -5.4002e-01, -4.4796e-01,\n",
      "          1.0069e-01,  2.6225e-01, -6.4234e-01, -6.1516e-02, -2.7117e-02,\n",
      "         -4.0514e-02, -3.8598e-02,  2.7053e-01, -6.4954e-01,  4.1456e-01,\n",
      "         -4.5710e-02, -2.0206e-01, -2.2806e-02,  2.9114e-01,  4.6451e-01,\n",
      "          5.7654e-01, -1.5372e-01, -5.5243e-02,  1.7777e-01, -8.0710e-02,\n",
      "         -4.9475e-01,  8.8572e-02,  6.1825e-01,  1.3250e-01,  3.3504e-01,\n",
      "          2.4322e-02,  4.4613e-01, -3.1316e-01, -1.5463e-01, -2.4710e-02,\n",
      "         -9.4860e-02,  2.1518e-01, -9.6295e-02,  3.9006e-02, -1.1539e-01,\n",
      "         -1.7409e-01, -3.5129e-01, -3.3133e-02, -4.3262e-01,  1.3936e-01,\n",
      "          5.5324e-01,  2.0913e-01,  3.8779e-01,  3.5261e-01,  1.8711e-01,\n",
      "          7.3043e-02, -1.6010e-01, -3.5512e-01, -1.0569e-01, -7.6102e-02,\n",
      "         -2.6371e-01,  2.4053e-01,  4.2738e-01, -6.0199e-01,  1.1770e-01,\n",
      "         -8.0219e-01, -1.7410e-01,  7.0150e-01,  6.8076e-02, -3.2507e-01,\n",
      "         -1.5822e-01, -1.1838e-01,  2.1977e-01, -9.8657e-02, -2.2582e-01,\n",
      "         -8.6755e-02, -2.9651e-01,  4.6392e-01, -2.2657e-01,  5.1312e-01,\n",
      "          6.9321e-02, -7.0538e-01, -4.3565e-01, -3.1650e-02, -1.2838e-01,\n",
      "         -2.0922e-01, -1.8200e-01,  2.1653e-01,  3.7738e-01, -1.0515e+00,\n",
      "         -1.0907e-01, -4.5800e-01,  8.3883e-03, -6.8988e-01,  6.0843e-01,\n",
      "          1.7627e-01, -4.2137e-01,  2.6988e-01,  2.8757e-01, -1.8240e-01,\n",
      "          3.1229e-01, -5.2345e-01, -1.5644e-01,  9.9696e-02, -1.9279e-01,\n",
      "         -3.4079e-01, -3.9772e-01,  1.2411e-01, -2.7896e-03,  4.3895e-01,\n",
      "          3.6986e-01,  7.2817e-02,  8.7313e-02,  2.4033e-01, -3.9403e-01,\n",
      "          3.5079e-01,  7.3673e-01, -1.4241e-01,  6.0261e-01, -1.9472e-01,\n",
      "          2.3583e-01, -1.5848e-01, -2.0075e-01,  3.6244e-01,  2.9392e-01,\n",
      "          5.8155e-04, -5.1315e-01, -2.1607e-01,  3.1884e-01,  1.0645e-01,\n",
      "         -2.5342e-01,  1.0399e-01,  2.2158e-01, -4.3493e-01,  3.9915e-01,\n",
      "         -5.2821e-02, -2.9857e-01,  8.2977e-01, -3.5982e-01, -1.0516e-01,\n",
      "          4.8300e-01,  1.0765e-01,  2.8372e-01, -3.3147e-01,  3.7817e-01,\n",
      "          2.0677e-01,  2.8866e-01,  3.5541e-02,  1.2486e-01,  5.1850e-02,\n",
      "         -2.3419e-01, -9.9559e-02]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "raw_image = Image.open(\"demo.png\")\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "features = model.get_image_features(inputs['pixel_values'])\n",
    "print(f\"inputs['pixel_values']: {inputs['pixel_values'].shape}\")\n",
    "# print(f\"inputs['pixel_values']: {inputs['pixel_values'][0]}\")\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model will be loaded from ..\\blip_vqa_base\\blip_vision_model.xml\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "\n",
    "VISION_MODEL_OV = Path(\"../blip_vqa_base/blip_vision_model.xml\")\n",
    "vision_model = model.vision_model\n",
    "vision_model.eval()\n",
    "\n",
    "# check that model works and save it outputs for reusage as text encoder input\n",
    "with torch.no_grad():\n",
    "    vision_outputs = vision_model(inputs[\"pixel_values\"])\n",
    "\n",
    "# if openvino model does not exist, convert it to IR\n",
    "if not VISION_MODEL_OV.exists():\n",
    "    # export pytorch model to ov.Model\n",
    "    with torch.no_grad():\n",
    "        ov_vision_model = ov.convert_model(vision_model, example_input=inputs[\"pixel_values\"])\n",
    "    # save model on disk for next usages\n",
    "    ov.save_model(ov_vision_model, VISION_MODEL_OV)\n",
    "    print(f\"Vision model successfuly converted and saved to {VISION_MODEL_OV}\")\n",
    "else:\n",
    "    print(f\"Vision model will be loaded from {VISION_MODEL_OV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection model will be loaded from ..\\blip_vqa_base\\blip_vision_proj_model.xml\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "\n",
    "PROJECTION_MODEL_OV = Path(\"../blip_vqa_base/blip_vision_proj_model.xml\")\n",
    "visual_projection_model = model.visual_projection\n",
    "visual_projection_model.eval()\n",
    "\n",
    "vision_outputs = model.vision_model(pixel_values=inputs[\"pixel_values\"], return_dict=None)\n",
    "pooled_output = vision_outputs[1]  # pooled_output\n",
    "\n",
    "# check that model works and save it outputs for reusage as text encoder input\n",
    "with torch.no_grad():\n",
    "    vision_outputs = visual_projection_model(pooled_output)\n",
    "\n",
    "# if openvino model does not exist, convert it to IR\n",
    "if not PROJECTION_MODEL_OV.exists():\n",
    "    # export pytorch model to ov.Model\n",
    "    with torch.no_grad():\n",
    "        ov_vision_projection_model = ov.convert_model(visual_projection_model, example_input=pooled_output)\n",
    "    # save model on disk for next usages\n",
    "    ov.save_model(ov_vision_projection_model, PROJECTION_MODEL_OV)\n",
    "    print(f\"Projection model successfuly converted and saved to {PROJECTION_MODEL_OV}\")\n",
    "else:\n",
    "    print(f\"Projection model will be loaded from {PROJECTION_MODEL_OV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "ov_vision_model = core.compile_model(VISION_MODEL_OV, 'CPU')\n",
    "ov_vision_projection_model = core.compile_model(PROJECTION_MODEL_OV, 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 384])\n",
      "tensor([[[ 0.8647,  0.9230,  0.9376,  ...,  1.7552,  1.7552,  1.7552],\n",
      "         [ 0.9084,  0.9376,  0.9522,  ...,  1.7552,  1.7552,  1.7552],\n",
      "         [ 0.9376,  0.9376,  0.9668,  ...,  1.7552,  1.7552,  1.7552],\n",
      "         ...,\n",
      "         [-0.7850, -0.7850, -0.7266,  ..., -0.3178, -0.2740, -0.3616],\n",
      "         [-0.7558, -0.7558, -0.7412,  ..., -0.3178, -0.3616, -0.4346],\n",
      "         [-0.7558, -0.7704, -0.7850,  ..., -0.3616, -0.4346, -0.4784]],\n",
      "\n",
      "        [[ 1.2194,  1.2495,  1.2795,  ...,  1.8948,  1.8948,  1.8948],\n",
      "         [ 1.2344,  1.2645,  1.2945,  ...,  1.8948,  1.8948,  1.8948],\n",
      "         [ 1.2495,  1.2795,  1.3095,  ...,  1.8948,  1.8948,  1.8948],\n",
      "         ...,\n",
      "         [-0.5965, -0.5965, -0.5515,  ..., -0.4014, -0.3264, -0.4164],\n",
      "         [-0.5665, -0.5665, -0.5515,  ..., -0.3864, -0.4164, -0.4914],\n",
      "         [-0.5665, -0.5815, -0.5965,  ..., -0.4164, -0.4764, -0.5365]],\n",
      "\n",
      "        [[ 1.2927,  1.3211,  1.3496,  ...,  1.9753,  1.9753,  1.9753],\n",
      "         [ 1.3211,  1.3354,  1.3638,  ...,  1.9753,  1.9753,  1.9753],\n",
      "         [ 1.3354,  1.3496,  1.3780,  ...,  1.9753,  1.9753,  1.9753],\n",
      "         ...,\n",
      "         [-0.3568, -0.3426, -0.2857,  ..., -0.2573, -0.2146, -0.2857],\n",
      "         [-0.3142, -0.3000, -0.3000,  ..., -0.2573, -0.2857, -0.3568],\n",
      "         [-0.3142, -0.3426, -0.3568,  ..., -0.3000, -0.3568, -0.3995]]])\n"
     ]
    }
   ],
   "source": [
    "pixel_values = inputs['pixel_values']\n",
    "print(pixel_values.shape)\n",
    "print(pixel_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8646511  0.9230448  0.93764323 0.95224166 0.99603695 1.0252337\n",
      " 1.0398322  1.0690291  1.0836275  1.1128243  1.1274228  1.1566195\n",
      " 1.171218   1.171218   1.2004149  1.2150133  1.2296118  1.2588086\n",
      " 1.273407   1.273407   1.3026038  1.3026038  1.3463992  1.3463992\n",
      " 1.3755959  1.3901944  1.4047928  1.4047928  1.4485881  1.4631865\n",
      " 1.477785   1.4923834  1.5069818  1.5069818  1.5069818  1.5215802\n",
      " 1.5215802  1.5361787  1.5507771  1.5507771  1.5507771  1.5653756\n",
      " 1.5653756  1.5799739  1.5799739  1.5945723  1.6091708  1.5945723\n",
      " 1.6091708  1.6091708  1.6237692  1.6383677  1.6383677  1.652966\n",
      " 1.6383677  1.652966   1.652966   1.6675645  1.6675645  1.6821629\n",
      " 1.6675645  1.6821629  1.6821629  1.6967614  1.7113597  1.7113597\n",
      " 1.6967614  1.7113597  1.7259582  1.7259582  1.7259582  1.7405566\n",
      " 1.7405566  1.7405566  1.7405566  1.7551551  1.7551551  1.7551551\n",
      " 1.7551551  1.7697535  1.7697535  1.7697535  1.7697535  1.7697535\n",
      " 1.7697535  1.7697535  1.7697535  1.7697535  1.7697535  1.7697535\n",
      " 1.7843518  1.7843518  1.7843518  1.7843518  1.7843518  1.7843518\n",
      " 1.7843518  1.7843518  1.7843518  1.7843518  1.7989503  1.7843518\n",
      " 1.7989503  1.7989503  1.7989503  1.7989503  1.7989503  1.8135487\n",
      " 1.8135487  1.8135487  1.7989503  1.8135487  1.8135487  1.8135487\n",
      " 1.8135487  1.8135487  1.8135487  1.8135487  1.8281472  1.8281472\n",
      " 1.8135487  1.8281472  1.8281472  1.8281472  1.8281472  1.8281472\n",
      " 1.8281472  1.8427455  1.8427455  1.8427455  1.8281472  1.8427455\n",
      " 1.8427455  1.8427455  1.8427455  1.8281472  1.8427455  1.857344\n",
      " 1.8427455  1.8427455  1.857344   1.857344   1.857344   1.857344\n",
      " 1.857344   1.857344   1.857344   1.857344   1.857344   1.8719424\n",
      " 1.8719424  1.857344   1.857344   1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8865409  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.857344\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.8719424  1.8719424  1.8719424  1.8719424  1.8719424  1.8719424\n",
      " 1.857344   1.857344   1.857344   1.857344   1.857344   1.857344\n",
      " 1.857344   1.857344   1.857344   1.857344   1.857344   1.857344\n",
      " 1.857344   1.857344   1.857344   1.857344   1.857344   1.857344\n",
      " 1.857344   1.8427455  1.857344   1.857344   1.857344   1.857344\n",
      " 1.857344   1.857344   1.857344   1.857344   1.8427455  1.8427455\n",
      " 1.857344   1.857344   1.857344   1.8427455  1.8427455  1.8427455\n",
      " 1.8427455  1.8427455  1.8427455  1.8427455  1.8427455  1.8427455\n",
      " 1.8427455  1.8427455  1.8281472  1.8427455  1.8427455  1.8427455\n",
      " 1.8281472  1.8427455  1.8427455  1.8427455  1.8281472  1.8281472\n",
      " 1.8281472  1.8427455  1.8281472  1.8281472  1.8281472  1.8281472\n",
      " 1.8281472  1.8281472  1.8281472  1.8281472  1.8281472  1.8281472\n",
      " 1.8281472  1.8281472  1.8281472  1.8281472  1.8281472  1.8281472\n",
      " 1.8135487  1.8135487  1.8135487  1.8135487  1.8135487  1.8135487\n",
      " 1.8135487  1.8135487  1.8135487  1.8135487  1.8135487  1.8135487\n",
      " 1.8135487  1.7989503  1.8135487  1.8135487  1.8135487  1.8135487\n",
      " 1.7989503  1.7989503  1.7989503  1.7989503  1.7989503  1.7989503\n",
      " 1.7989503  1.7989503  1.7843518  1.7843518  1.7843518  1.7843518\n",
      " 1.7843518  1.7843518  1.7843518  1.7843518  1.7843518  1.7843518\n",
      " 1.7843518  1.7843518  1.7843518  1.7697535  1.7697535  1.7697535\n",
      " 1.7697535  1.7697535  1.7697535  1.7697535  1.7551551  1.7551551\n",
      " 1.7697535  1.7697535  1.7697535  1.7551551  1.7551551  1.7551551 ]\n"
     ]
    }
   ],
   "source": [
    "def blip_preprocess(image: Image) -> np.ndarray:\n",
    "    # convert to rgb\n",
    "    image = image.convert(\"RGB\")\n",
    "    # resize to (384, 384)\n",
    "    image = image.resize((384, 384), resample=Image.Resampling.BICUBIC)\n",
    "    image.save(\"demo_resized.png\")\n",
    "    image = Image.open(\"demo_resized.png\")\n",
    "    image = np.array(image)\n",
    "    # resacle (1 / 255)\n",
    "    image = image / 255.0\n",
    "    image = image.astype(np.float32)\n",
    "    # normalize\n",
    "    OPENAI_CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\n",
    "    OPENAI_CLIP_STD = [0.26862954, 0.26130258, 0.27577711]\n",
    "    mean = np.array(OPENAI_CLIP_MEAN).astype(image.dtype)\n",
    "    std = np.array(OPENAI_CLIP_STD).astype(image.dtype)\n",
    "\n",
    "    image = (image - mean) / std\n",
    "    # convert to channel first (num_channels, height, width)\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    return image\n",
    "\n",
    "custom_preprocess = blip_preprocess(raw_image)\n",
    "\n",
    "res = np.expand_dims(custom_preprocess, axis=0)\n",
    "print(res[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384, 384)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(custom_preprocess.shape)\n",
    "preprocess_diff = custom_preprocess - pixel_values.numpy()\n",
    "preprocess_diff = np.max(preprocess_diff)\n",
    "print(preprocess_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 384, 384)\n"
     ]
    }
   ],
   "source": [
    "custom_preprocess = np.expand_dims(custom_preprocess, axis=0)\n",
    "print(custom_preprocess.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "[-0.3295024  -0.26385215 -0.0980585  -0.61664826 -0.25132835 -0.81028205\n",
      "  0.8470111   0.04173357  0.5050232   0.30850348]\n",
      "(1, 512)\n",
      "[[ 5.43425918e-01 -1.02331668e-01 -4.19514924e-01 -7.79112149e-03\n",
      "  -7.34343827e-02 -7.73895681e-02 -4.57431786e-02  3.97739679e-01\n",
      "   1.76487401e-01 -2.06602097e-01  2.76051819e-01 -4.14276086e-02\n",
      "   3.32250893e-01  1.51612788e-01 -6.69413898e-03 -1.32611036e-01\n",
      "  -1.00314453e-01  1.59211293e-01 -6.27769902e-02 -3.53940874e-01\n",
      "  -1.24040239e-01  6.40376210e-01  4.15159315e-01 -9.96035218e-01\n",
      "   2.98053145e-01  2.36027554e-01 -5.80286205e-01  8.01462412e-01\n",
      "  -3.58247548e-01 -1.09790623e-01 -3.15915465e-01 -6.72160566e-01\n",
      "  -6.99690402e-01 -7.97545314e-02  1.46668889e-02 -1.49127794e-02\n",
      "   1.61959171e-01  3.87137145e-01  1.58338659e-02 -2.83590466e-01\n",
      "  -3.01553845e-01 -2.01093599e-01  1.56159341e-01 -1.74015045e-01\n",
      "  -2.35688895e-01 -1.45444289e-01  3.14966381e-01  6.97640359e-01\n",
      "   3.42270210e-02  9.42795873e-02  5.47776997e-01  1.32348001e-01\n",
      "  -1.20208263e-01 -2.62447953e-01 -2.33268365e-01 -5.17575145e-01\n",
      "   3.05951297e-01  5.62052846e-01 -5.55257440e-01  1.92489456e-02\n",
      "  -1.50578737e-01  4.32623655e-01  3.44950527e-01 -1.82370767e-01\n",
      "   9.92712844e-03 -2.38852948e-01  5.45067847e-01  2.97190279e-01\n",
      "   9.66582596e-02 -1.42790034e-01  3.22043151e-01 -3.59609008e-01\n",
      "  -2.01667413e-01 -1.93549454e-01  4.39139679e-02  8.78493208e-03\n",
      "  -4.23381813e-02 -2.61736989e-01 -3.11232239e-01 -8.84353399e-01\n",
      "  -1.92093909e-01 -1.07115634e-01 -3.65328230e-02  2.30388194e-02\n",
      "  -4.34968360e-02 -3.18750858e-01 -3.35662931e-01 -4.80639249e-01\n",
      "   7.82205313e-02 -3.57456952e-01 -4.44634408e-01 -2.62813359e-01\n",
      "  -4.47571099e-01 -2.56336123e-01  6.04271144e-02 -7.09729493e-02\n",
      "   2.70532548e-01  9.27305520e-02  1.42698288e-01 -9.99860644e-01\n",
      "   3.98462385e-01 -2.20841423e-01  2.66485363e-01 -2.70296663e-01\n",
      "   7.62727782e-02 -2.40749002e-01  2.32332900e-01 -2.30682418e-02\n",
      "   6.11135662e-01  5.47358533e-03  5.29099762e-01 -1.53380767e-01\n",
      "   7.80158043e-02  3.29043806e-01 -2.73624837e-01 -4.75042522e-01\n",
      "  -3.37745309e-01 -7.06242844e-02 -2.36884773e-01  3.79763871e-01\n",
      "  -1.34832695e-01  2.31908351e-01  1.58756435e-01  8.21775496e-02\n",
      "  -4.02828343e-02 -4.53178793e-01 -2.34368846e-01 -1.48057073e-01\n",
      "   1.70525387e-01 -1.78665621e-03 -3.92171651e-01 -3.44502240e-01\n",
      "  -7.48739779e-01  1.39529660e-01 -9.73212272e-02  6.79826975e-01\n",
      "  -6.13390394e-02  9.95700657e-02 -3.58781546e-01  9.29191411e-02\n",
      "   5.52584529e-02 -3.87757301e-01 -4.09639567e-01  2.38821551e-01\n",
      "  -5.89243054e-01  4.94011432e-01  3.08129191e-01 -7.38382265e-02\n",
      "   6.70161322e-02  3.02237153e-01 -4.94627982e-01  1.43305853e-01\n",
      "   4.49416399e-01 -7.42445946e-01 -7.28899896e-01  9.39811021e-02\n",
      "   5.83744459e-02 -3.19210321e-01 -4.31894600e-01 -2.74613053e-01\n",
      "  -3.54937106e-01  1.66470066e-01  3.90361965e-01  5.62400579e-01\n",
      "   1.00786977e-01 -1.11653991e-01 -4.75274809e-02 -2.63944596e-01\n",
      "   1.19375162e-01  6.97962463e-01  3.51860315e-01  6.41511083e-01\n",
      "   1.43756136e-01 -5.72311044e-01  1.87645063e-01 -6.50773287e-01\n",
      "   1.21350251e-01  1.91892684e-01 -4.77922037e-02  4.06991807e-04\n",
      "   3.08259368e-01 -2.65666038e-01 -1.29774719e-01  1.97649285e-01\n",
      "   1.84621409e-01 -4.54970747e-01  3.37489754e-01  3.17342699e-01\n",
      "   1.08760841e-01  3.36266100e-01 -3.34571987e-01  3.16742897e-01\n",
      "  -2.65419543e-01 -4.64219265e-02 -9.86051410e-02  2.12689176e-01\n",
      "  -5.21720648e-02  2.22785845e-01 -2.03451645e-02  4.32571709e-01\n",
      "  -9.27819014e-02  1.17080998e+00  5.12759745e-01  4.15067226e-02\n",
      "   1.37579843e-01 -4.81996238e-01  4.01494920e-01  1.67010054e-01\n",
      "   3.41859348e-02  2.90746450e-01  2.30075493e-01 -5.20796031e-02\n",
      "   3.49619418e-01  1.01335712e-01 -9.80236009e-02  2.84386247e-01\n",
      "   1.69745356e-01  3.01387399e-01 -6.81584999e-02  2.66178012e-01\n",
      "   5.30676208e-02 -1.30886734e-01  3.76887530e-01 -9.14023742e-02\n",
      "   3.22922468e-01  1.52189985e-01  4.31228787e-01 -8.27552676e-01\n",
      "   4.78768647e-01 -3.63715500e-01  3.52684736e-01 -3.03692430e-01\n",
      "   2.55063683e-01  6.70580804e-01 -3.40973437e-01  4.52283025e-02\n",
      "  -1.50789991e-01 -7.26582170e-01 -4.41118270e-01  1.73347685e-02\n",
      "   1.65464520e-01 -1.76951259e-01 -9.49182436e-02 -4.44457620e-01\n",
      "   8.72697383e-02 -4.69093323e-01  4.70248461e-01 -6.55152142e-01\n",
      "  -2.70232379e-01  5.32309234e-01  1.96166366e-01  1.63661823e-01\n",
      "  -6.97338164e-01 -1.87104300e-01 -3.02439481e-01  3.45859796e-01\n",
      "   2.19968975e-01 -1.82028159e-01  1.06273323e-01 -3.65524739e-01\n",
      "   3.20386052e-01 -1.02832451e-01 -4.90544528e-01  7.39960521e-02\n",
      "  -4.79159564e-01  1.26357913e-01  1.51646644e-01 -2.06945196e-01\n",
      "  -1.45253092e-02  2.06472963e-01 -1.00341886e-01  7.39237368e-01\n",
      "  -5.64485192e-02  2.40796953e-01 -1.08489826e-01 -1.04955822e-01\n",
      "   8.58749270e-01  1.00053705e-01  1.30254611e-01 -2.06086710e-01\n",
      "   1.88427120e-01  3.97993624e-02 -7.97991455e-01 -3.35135400e-01\n",
      "  -1.90365449e-01 -1.75190624e-02 -2.10077548e-03 -1.20622583e-01\n",
      "  -3.82948220e-01  1.85561910e-01 -4.41350341e-01  6.69291615e-01\n",
      "  -3.45090702e-02 -6.26109466e-02  4.25760567e-01  4.66725260e-01\n",
      "  -3.93508039e-02  5.11230528e-01  6.36952519e-01 -2.60509610e-01\n",
      "   6.52489737e-02  4.50245896e-03 -1.12158060e-01  5.02310872e-01\n",
      "  -2.31166750e-01  1.37533441e-01 -3.51020694e-01 -5.06546319e-01\n",
      "  -3.55376989e-01  1.49792761e-01  2.68691033e-01  2.17640832e-01\n",
      "  -4.54466045e-02  2.23472536e-01  4.27786291e-01 -2.45187610e-01\n",
      "   2.17220977e-01  1.67057347e-02 -3.69729310e-01  6.53889552e-02\n",
      "  -6.95880890e-01 -4.35176790e-01 -1.11125067e-01  7.76803911e-01\n",
      "   2.28273332e-01 -3.47970188e-01  1.16908483e-01 -2.12267935e-01\n",
      "  -1.47662973e-02  1.15493931e-01  5.29679477e-01  1.50811583e-01\n",
      "   5.88085316e-03  5.94347669e-03 -6.57922626e-01  1.24747477e-01\n",
      "  -2.97902286e-01  5.75368889e-02 -4.97883528e-01 -3.65882844e-01\n",
      "  -2.99713373e-01 -4.70194131e-01 -1.37388512e-01 -3.91609401e-01\n",
      "  -4.28683817e-01  1.63133755e-01 -5.25569916e-01  1.91575602e-01\n",
      "   2.56228268e-01  1.64373428e-01 -2.02970412e-02 -3.04203689e-01\n",
      "  -1.09177113e-01 -3.46862823e-01  1.24787420e-01  2.38102913e-01\n",
      "   2.88394004e-01 -5.90348065e-01  9.92547944e-02 -3.43813837e-01\n",
      "  -4.92149480e-02  7.30822906e-02 -1.91637129e-01 -2.81350046e-01\n",
      "   5.04672118e-02 -1.02149835e-02  3.48615617e-01 -1.12377428e-01\n",
      "  -5.40354788e-01 -4.47971225e-01  1.00600079e-01  2.62494296e-01\n",
      "  -6.42557621e-01 -6.10067323e-02 -2.73440909e-02 -4.05357592e-02\n",
      "  -3.82115208e-02  2.70115525e-01 -6.49065197e-01  4.14581925e-01\n",
      "  -4.56467941e-02 -2.01811194e-01 -2.26834305e-02  2.91338980e-01\n",
      "   4.64307517e-01  5.76465786e-01 -1.53970525e-01 -5.51525354e-02\n",
      "   1.77452087e-01 -8.06071907e-02 -4.94650185e-01  8.83403346e-02\n",
      "   6.18395925e-01  1.32336289e-01  3.34999979e-01  2.44763140e-02\n",
      "   4.46197152e-01 -3.13350767e-01 -1.54526532e-01 -2.46023703e-02\n",
      "  -9.51291546e-02  2.15372160e-01 -9.60108638e-02  3.84804346e-02\n",
      "  -1.15761243e-01 -1.74084708e-01 -3.51246834e-01 -3.29681225e-02\n",
      "  -4.32824969e-01  1.39818504e-01  5.53606212e-01  2.09225371e-01\n",
      "   3.87734056e-01  3.52710098e-01  1.87229961e-01  7.29593188e-02\n",
      "  -1.60068184e-01 -3.55348200e-01 -1.05614983e-01 -7.62584880e-02\n",
      "  -2.63829738e-01  2.40909129e-01  4.27270621e-01 -6.01816714e-01\n",
      "   1.17837146e-01 -8.01822662e-01 -1.74614817e-01  7.01747477e-01\n",
      "   6.84325397e-02 -3.24970990e-01 -1.58350736e-01 -1.18225411e-01\n",
      "   2.19604045e-01 -9.88980308e-02 -2.26063237e-01 -8.68592113e-02\n",
      "  -2.96702355e-01  4.63631153e-01 -2.26278260e-01  5.13322473e-01\n",
      "   6.93576559e-02 -7.05398917e-01 -4.35577631e-01 -3.16133238e-02\n",
      "  -1.28228933e-01 -2.09079593e-01 -1.82071134e-01  2.16218174e-01\n",
      "   3.77201796e-01 -1.05173981e+00 -1.09216407e-01 -4.57986683e-01\n",
      "   8.47363751e-03 -6.89329743e-01  6.08457506e-01  1.76559061e-01\n",
      "  -4.20902282e-01  2.69896150e-01  2.87724197e-01 -1.82623401e-01\n",
      "   3.12359929e-01 -5.23316443e-01 -1.56471834e-01  9.95279998e-02\n",
      "  -1.92854062e-01 -3.40672940e-01 -3.97308171e-01  1.23986378e-01\n",
      "  -2.96002976e-03  4.39354748e-01  3.69537324e-01  7.23640546e-02\n",
      "   8.72730836e-02  2.40688175e-01 -3.93947482e-01  3.50801528e-01\n",
      "   7.36767769e-01 -1.42283186e-01  6.02489650e-01 -1.95000499e-01\n",
      "   2.35812128e-01 -1.58379003e-01 -2.01009259e-01  3.62804174e-01\n",
      "   2.94125021e-01  3.37535108e-04 -5.13183773e-01 -2.16744989e-01\n",
      "   3.19092512e-01  1.06701113e-01 -2.53352791e-01  1.04254812e-01\n",
      "   2.21719325e-01 -4.34588820e-01  3.99221092e-01 -5.28366901e-02\n",
      "  -2.98880666e-01  8.29683721e-01 -3.59632075e-01 -1.05128989e-01\n",
      "   4.83447671e-01  1.07783534e-01  2.83457518e-01 -3.31325620e-01\n",
      "   3.78502011e-01  2.06823304e-01  2.88403004e-01  3.52683663e-02\n",
      "   1.24566928e-01  5.20656034e-02 -2.34784126e-01 -9.93320197e-02]]\n"
     ]
    }
   ],
   "source": [
    "vision_model_out = ov_vision_model.output(1)\n",
    "pooled_embed = ov_vision_model(custom_preprocess)[vision_model_out]\n",
    "print(pooled_embed.shape)\n",
    "print(pooled_embed[0][:10])\n",
    "\n",
    "image_features = ov_vision_projection_model(pooled_embed)[ov_vision_projection_model.output(0)]\n",
    "print(image_features.shape)\n",
    "print(image_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
